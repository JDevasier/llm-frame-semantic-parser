{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "prompts = pd.read_csv('fn1.7-test-prompts.csv')\n",
    "\n",
    "# predictions = pd.read_json('batch-gpt-4o-mini-0.0temperature-json-existing-cot-predictions-cleaned.jsonl', lines=True)\n",
    "with open('batch-gpt-4o-mini-0.0temperature-json-complete-cot-predictions-cleaned.jsonl') as f:\n",
    "    predictions = f.readlines()\n",
    "\n",
    "predictions = [json.loads(p.strip()) for p in predictions]\n",
    "\n",
    "prompts['prediction'] = predictions\n",
    "predictions = prompts[['output', 'prediction', 'frame_elements']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 6204\n",
      "Missed: 4702\n",
      "FP: 53852\n",
      "FN: 4745\n",
      "Precision (ignoring near misses): 0.11207862123785092\n",
      "Precision: 0.10330358332223258\n",
      "Recall: 0.5666270892318933\n",
      "F1: 0.17474825716498837\n",
      "Accuracy: 0.09573926328297402\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "near_miss = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0 # not really used because tags only have positive values, technically all other FEs are TN, but we don't really care about that (maybe we should?)\n",
    "\n",
    "for label, pred, fes in predictions[['output', 'prediction', 'frame_elements']].values:\n",
    "\n",
    "    # Get each predicted FE span from the prediction\n",
    "    pred_tags = pred\n",
    "    real_tags = eval(fes)\n",
    "\n",
    "    # Check each predicted FE span\n",
    "    for tag, content in pred_tags.items():\n",
    "        if tag.capitalize() in real_tags:\n",
    "            if content == real_tags[tag.capitalize()]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                near_miss += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    # Check each real FE span\n",
    "    for tag, content in real_tags.items():\n",
    "        if tag not in pred_tags:\n",
    "            fn += 1\n",
    "        elif content != pred_tags[tag]:\n",
    "            fn += 1\n",
    "\n",
    "# print(f'Perfect: {perfect} out of {len(predictions)}')\n",
    "print(f'TP: {tp}')\n",
    "print(f'Missed: {near_miss}')\n",
    "print(f'FP: {fp}')\n",
    "print(f'FN: {fn}')\n",
    "\n",
    "print(f'Precision (ignoring near misses): {tp / (tp + fp - near_miss)}')\n",
    "print(f'Precision: {tp / (tp + fp)}')\n",
    "print(f'Recall: {tp / (tp + fn)}')\n",
    "print(f'F1: {2 * tp / (2 * tp + fp + fn)}')\n",
    "\n",
    "print(f'Accuracy: {tp / (tp + fn + fp)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
