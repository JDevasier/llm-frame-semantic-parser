{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91cd049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22c41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.dirname('/home/jdd5089/projects/llm-fsp/finetune-notebooks/json-existing/phi-4/vllm-inference.ipynb')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624b98d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 13:09:42 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 08-07 13:09:43 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 08-07 13:09:43 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 08-07 13:09:43 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='unsloth/phi-4-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/phi-4-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=unsloth/phi-4-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 08-07 13:09:44 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x706aeb9af850>\n",
      "ERROR 08-07 13:09:44 [core.py:396] EngineCore failed to start.\n",
      "ERROR 08-07 13:09:44 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 08-07 13:09:44 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 08-07 13:09:44 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 08-07 13:09:44 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 08-07 13:09:44 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 08-07 13:09:44 [core.py:396]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 08-07 13:09:44 [core.py:396]     self._init_executor()\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n",
      "ERROR 08-07 13:09:44 [core.py:396]     self.collective_rpc(\"init_device\")\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 08-07 13:09:44 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 08-07 13:09:44 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 08-07 13:09:44 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 08-07 13:09:44 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 604, in init_device\n",
      "ERROR 08-07 13:09:44 [core.py:396]     self.worker.init_device()  # type: ignore\n",
      "ERROR 08-07 13:09:44 [core.py:396]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 125, in init_device\n",
      "ERROR 08-07 13:09:44 [core.py:396]     torch.cuda.set_device(self.device)\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "ERROR 08-07 13:09:44 [core.py:396]     torch._C._cuda_setDevice(device)\n",
      "ERROR 08-07 13:09:44 [core.py:396]   File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
      "ERROR 08-07 13:09:44 [core.py:396]     raise RuntimeError(\n",
      "ERROR 08-07 13:09:44 [core.py:396] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n",
      "    self.collective_rpc(\"init_device\")\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 604, in init_device\n",
      "    self.worker.init_device()  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 125, in init_device\n",
      "    torch.cuda.set_device(self.device)\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/jdd5089/miniconda3/envs/llm/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m lora_adapter_path = \u001b[33m\"\u001b[39m\u001b[33m./phi-4-fsp-ft-cand\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Initialize vLLM with LoRA adapter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_lora\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"float16\" for GPU, \"bfloat16\" if supported\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"cpu\" if no GPU\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/llm_engine.py:510\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    508\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    111\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     89\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:73\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:494\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    493\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:398\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._init_core_engines(vllm_config, new_core_engine,\n\u001b[32m    395\u001b[39m                         \u001b[38;5;28mself\u001b[39m.resources.core_engines)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:430\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] != sync_input_socket:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m eng_id_bytes, msg = sync_input_socket.recv_multipart()\n\u001b[32m    434\u001b[39m eng_id = \u001b[38;5;28mint\u001b[39m.from_bytes(eng_id_bytes, byteorder=\u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above."
     ]
    }
   ],
   "source": [
    "# vLLM offline inference with LoRA adapter on phi-4\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "\n",
    "MAX_TOKENS = 4096  # Maximum tokens for generation\n",
    "LORA_PATH = \"phi-4-fsp-ft-cand\"  # Path to LoRA adapter\n",
    "\n",
    "frame_descriptions = pd.read_csv('/home/jdd5089/projects/llm-fsp/finetune-notebooks/json-existing/frame_descriptions_json.csv')\n",
    "frame_descriptions_mapping = frame_descriptions[['name', 'description']].set_index('name').to_dict()['description']\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"### Task:\n",
    "You are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \n",
    "\n",
    "### Notes:\n",
    "- Return the tagged sentence in a ```json ``` code block.\n",
    "- Texts must not overlap.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"### Frame Information:\n",
    "{frame_info}\n",
    "\n",
    "### Input:\n",
    "{input_sentence}\n",
    "\"\"\"\n",
    "\n",
    "# Path to base model and LoRA adapter\n",
    "base_model_path = \"unsloth/phi-4-unsloth-bnb-4bit\"\n",
    "lora_adapter_path = \"./phi-4-fsp-ft-cand\"\n",
    "\n",
    "\n",
    "# Initialize vLLM with LoRA adapter\n",
    "llm = LLM(\n",
    "    model=base_model_path,\n",
    "    enable_lora=True,\n",
    "    dtype=\"auto\",  # or \"float16\" for GPU, \"bfloat16\" if supported\n",
    "    device=\"cuda\"  # or \"cpu\" if no GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from candidate_identifier import LexicalUnitManager\n",
    "\n",
    "lum = LexicalUnitManager()\n",
    "lum.load_lus()\n",
    "\n",
    "def get_candidates(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Get candidate lexical units from the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing a target-highlighted string and \n",
    "        its corresponding candidate frames.\n",
    "    \"\"\"\n",
    "    candidates = lum.lookup_lus(text)\n",
    "    \n",
    "    result = []\n",
    "    for span, cand_frames in candidates.items():\n",
    "        highlighted_text = f\"{text[:span[0]]}**{text[span[0]:span[1]]}**{text[span[1]:]}\"\n",
    "        result.append({\n",
    "            \"target\": highlighted_text,\n",
    "            \"candidates\": cand_frames\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "# text = \"The bank will not lend money to the company.\"\n",
    "# candidates = get_candidates(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "fsp_lora = LoRARequest(\"fsp_lora\", 1, LORA_PATH)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-4\")\n",
    "\n",
    "def frame_semantic_parse(text: str, tie_breaker = \"random\") -> dict:\n",
    "    \"\"\"\n",
    "    Perform frame semantic parsing on the input text using vLLM with LoRA adapter.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to analyze.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    candidates = get_candidates(text)\n",
    "    \n",
    "    # Create prompts for each candidate\n",
    "    user_prompts = []\n",
    "    for candidate in candidates:\n",
    "        target = candidate['target']\n",
    "        for frame in candidate['candidates']:\n",
    "            frame_info = frame_descriptions_mapping[frame]\n",
    "            prompt = user_prompt.format(frame_info=frame_info, input_sentence=target)\n",
    "            user_prompts.append(prompt)\n",
    "    \n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": user_prompt}] for user_prompt in user_prompts]\n",
    "    \n",
    "    messages = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    print(messages)\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    # Run inference\n",
    "    responses = llm.generate(\n",
    "        messages,\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=fsp_lora\n",
    "    )\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Example usage\n",
    "while True:\n",
    "    text = input(\"Enter a sentence for frame semantic parsing (or 'exit' to quit): \")\n",
    "    if text.lower() == 'exit':\n",
    "        break\n",
    "    if not text.strip():\n",
    "        print(\"Empty input. Please enter a valid sentence.\")\n",
    "        continue\n",
    "    response = frame_semantic_parse(text)\n",
    "\n",
    "    for i, res in enumerate(response):\n",
    "        print(f\"Response {i+1}:\")\n",
    "        for message in res.outputs:\n",
    "            if message.text:\n",
    "                print(message.text)\n",
    "            else:\n",
    "                print(\"No text output.\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    print(\"Done processing the input.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d00d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = ['<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Relational_natural_features\\nFrame Definition: The Focal_feature is defined in relation to a Landmark_feature, either as a particular part, or as an immediately bordering entity.\\n\\nFrame Elements:\\nConstituent_parts (Peripheral): Salient parts of the Focal_feature.\\nContainer_possessor (Extra-Thematic): The location that the Focal_feature is a part of.\\nFormational_cause (Extra-Thematic): Indicates the action (or causer) which brings the features of the Focal_feature about.\\nFocal_feature (Core): A stable bounded area defined by its relation to another landform. This FE is incorporated in each LU in this frame.\\nName (Extra-Thematic): This FE is used for the Names of Focal_features.\\nRelative_location (Peripheral): A place that a Focal_feature is located with respect to.\\nType (Peripheral): The Type is the subtype of natural feature.\\nDescriptor (Extra-Thematic): Any description, characteristic, or property of the Focal_feature which is not covered by more specific FEs.\\nLandmark_feature (Core): The landform with respect to which the Focal_feature is defined.\\n\\n\\n### Input:\\nThe **bank** will not lend money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Businesses\\nFrame Definition: A Proprietor owns or runs a Business which provides a Product (which may be goods or services).\\n\\nFrame Elements:\\nProduct (Peripheral): Product identifies the product or service provided by the business.\\nProprietor (Peripheral): Proprietor is the owner or chief participant in the business.\\nDescriptor (Peripheral): Descriptor indicates other characteristics of the business besides its Product and Proprietor.\\nPlace (Peripheral): This FE identifies the Place in which the business is located.\\nBusiness (Core): Business identifies the company or establishment. This may be the target itself or the Business_Name if it is an appositive not set off with commas (i.e. restrictive):\\nBusiness_name (Peripheral): The name of the Business when it appears in an appositive set off by commas (non-restrictive):\\nService_provider (Peripheral): When a Business is described in terms of the professionals who work to produce the product or service, use Service_provider.\\n\\n\\n### Input:\\nThe **bank** will not lend money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', \"<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Giving\\nFrame Definition: A Donor transfers a Theme from a Donor to a Recipient. This frame includes only actions that are initiated by the Donor (the one that starts out owning the Theme). Sentences (even metaphorical ones) must meet the following entailments: the Donor first has possession of the Theme. Following the transfer the Donor no longer has the Theme and the Recipient does.\\n\\nFrame Elements:\\nDonor (Core): The person that begins in possession of the Theme and causes it to be in the possession of the Recipient .\\nRecipient (Core): The entity that ends up in possession of the Theme.\\nTheme (Core): The object that changes ownership.\\nPlace (Peripheral): The Place where the Donor gives the Theme to the Recipient.\\nExplanation (Extra-Thematic): The Explanation for which the Donor gives the Theme to the Recipient.\\nTime (Peripheral): The Time is when the Donor gives the Theme to the Recipient.\\nPurpose (Peripheral): The Purpose for which the Donor gives the Theme to the Recipient.\\nMeans (Peripheral): The Means by which the Donor gives the Theme to the Recipient.\\nManner (Peripheral): The Manner is the manner in which the Donor gives the Theme to the Recipient.\\nCircumstances (Extra-Thematic): The Circumstances are the conditions under which the Theme is given.\\nImposed_purpose (Extra-Thematic): The Recipient's intended purpose for the Theme.\\nDepictive (Extra-Thematic): A description of the Donor, Recipient, or Theme given independently of the giving event per se.\\nPeriod_of_iterations (Extra-Thematic): The length of time from when the event denoted by the target began to be repeated to when it stopped.\\n\\n\\n### Input:\\nThe bank **will** not lend money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>\", '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Documents\\nFrame Definition: Words in the frame refer to any Document that has a legal status or conventional social significance. Some Documents empower the Bearer of the Document to execute the Right. Others indicate the Obligation of the Bearer. Still others show the identity or Status of the Bearer.\\n\\nFrame Elements:\\nBearer (Core): The holder of the document.\\nIssuer (Core): The authority that issues the document.\\nObligation (Core): Obligation is the obligation that is imposed on the Bearer of the Document.\\nRight (Core): Right is the right of the Bearer of the Document that is authorized by the Document.\\nStatus (Core): Status is the state of the Bearer of the Document indicated by the Document.\\nSpecification (Extra-Thematic): A Specification is a condition that applies to the Rights and/or Obligations described in the Document.\\nDocument (Core): Document is the piece of paper that has the legal status.\\nMedium (Peripheral): The language the Document is written in, or a specification of the kind of object that instantiates it.\\nDescriptor (Extra-Thematic): Descriptor covers any characterization or description of the target that is not covered by another more specific FE.\\n\\n\\n### Input:\\nThe bank **will** not lend money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', \"<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Desiring\\nFrame Definition: An Experiencer desires that an Event occur. (Note that commonly a resultant state of the Event will stand in for the Event.) In some cases, the Experiencer is an active participant in the Event, and in such cases the Event itself is often not mentioned, but rather some Focal_participant which is subordinately involved in the Event. Generally, the use of a word in this frame implies that the specific Event has not yet happened, but that the Experiencer believes that they would be happier if it did. Sometimes the Time_of_event, Purpose_of_event, or the Location_of_event are mentioned without the explicit mention of the Event.\\n\\nFrame Elements:\\nExperiencer (Core): The Experiencer is the person (or sentient being) who wishes for the Event to occur.\\nEvent (Core): The change that the Experiencer would like to see.\\nFocal_participant (Core): This is the entity that the Experiencer wishes to be affected by some Event.\\nDegree (Peripheral): The Degree is a the extent of the Experiencer's Desiring.\\nManner (Peripheral): The way in which the Experiencer desires something.\\nExplanation (Extra-Thematic): An aspect of the Experiencer that causes a desire.\\nPurpose_of_event (Peripheral): The Purpose_of_event is the purpose pertaining to the desired Event.\\nTime_of_event (Peripheral): The time that the Event would ideally occur.\\nLocation_of_event (Core): The Location_of_event is the place involved in the desired Event.\\nTime (Peripheral): The Time is when the Experiencer desires something.\\nPlace (Peripheral): Where the event takes place. Note that this FE is of extremely rare occurrence in this frame.\\nDuration (Peripheral): The amount of time for which the Experiencer has desired something.\\nRole_of_focal_participant (Peripheral): The Role is a role filled by a Focal_participant in the Event that the Experiencer desires.\\n\\n\\n### Input:\\nThe bank **will** not lend money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>\", '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Negation\\nFrame Definition: This frame models the semantic contribution of negation, in which the profiled content of the Negated_proposition is asserted to contrast incompatibly with a corresponding Factual_situation, which is normally implicit. The Negated_proposition and the Factual_situation share non-focal content, on the basis of which they correspond, and contrast in a subset of their content, the focal content, established with reference to a set of focus constructions not described here. (The focus of negation is often conveyed intonationally in spoken English; in text, however, negative focus must be derived inferentially from contextual reference. A subset of focus constructions will be annotated in the constructicon.)\\n\\nFrame Elements:\\nNegated_proposition (Core): The situation which is presented as false overall. The particular focus of negation which contrasts with the Factual_situation is not separately marked.\\nFactual_situation (Core): The situation or part of situation that actually pertains, in contradistinction to the profiled situation in the Negated_proposition that is asserted not to pertain. In spoken language and casual written language, but not in normatively edited written texts, the Factual_situation may be expressed as an unmarked clause immediately preceding or following the Negated_proposition.\\n\\n\\n### Input:\\nThe bank will **not** lend money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Lending\\nFrame Definition: The Lender gives the Theme to the Borrower with the expectation that the Borrower will return the Theme to the Lender after a Duration of time. This frame differs from the Borrowing frame in that this frame profiles the Lender in active sentences, whereas the Borrowing frame profiles the Borrower.\\n\\nFrame Elements:\\nBorrower (Core): The person or institution who receives the Theme from the Lender for a Duration.\\nDuration (Peripheral): The amout of time in which the Borrower has possession of the Theme.\\nLender (Core): The person or institution who gives the Theme to the Borrower for a Duration.\\nTheme (Core): The object that is transferred from the Lender to the Borrower for a Duration.\\nPurpose (Peripheral): The aim of the Lender which they believe will be accomplished by lending the Theme to the Borrower.\\nTime (Peripheral): The time when the lending event occurs.\\nManner (Peripheral): The way in which the Lender lends the Theme.\\nPlace (Peripheral): The location in which the Lender lends the Theme to the Borrower.\\n\\n\\n### Input:\\nThe bank will not **lend** money to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Money\\nFrame Definition: Money is a medium used for exchanging goods and services. It is in most cases issued by a government (its Creator) in the form of coins and banknotes. Particular amounts of it may have a designated or planned Use, or may have come from some Origin.\\n\\nFrame Elements:\\nMoney (Core): Money is used as a medium of exchange.\\nInherent_purpose (Extra-Thematic): The use for which the Money is intended or designed.\\nCreator (Extra-Thematic): The government (or possibly other organization) that mints, issues and guarantees the legal use of the Money.\\nTime_of_creation (Extra-Thematic): The time at which an Money comes into existence or begins to be circulated.\\nName (Extra-Thematic): The term used to refer to the Money.\\nType (Extra-Thematic): An indication of the subtype of Money.\\nMaterial (Extra-Thematic): Any indication of what makes up the Money, including components, ingredients, etc.\\nOrigin (Extra-Thematic): The process or means by which the Money comes to be possessed.\\nPossessor (Extra-Thematic): The Possessor has control over the use the Money.\\n\\n\\n### Input:\\nThe bank will not lend **money** to the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', \"<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Locative_relation\\nFrame Definition: A Figure is located relative to a Ground location.\\n\\nFrame Elements:\\nGround (Core): The Ground serves as a basis for describing the location of the Figure.\\nFigure (Core): The Figure is perceived as located relative to a certain Ground location. The figure can be an entity or an event.\\nDistance (Extra-Thematic): This FE identifies the Distance between the Figure and the Ground. This FE requires an image schema with a vector that extends from the Ground, or a subpart of the Ground, in some direction, and the Distance is measured along this vector from the Ground to the profiled region. In cases where the target does not introduce such an image schema, Distance should be annotated as part of the Direction FE that introduces such a vector. Note that a number of frequent adverbial targets, e.g. up.adv and down.adv, identify the Ground, with the location of an implicit viewer, so Distance is effectively measured from the viewpoint location in these cases.\\nDirection (Extra-Thematic): This FE identifies the Direction from a reference location (generally, the deictic center) of a path to the Figure. This FE is filled by one of a closed class of adverbs: up, down, out (meaning 'beyond the boundary around the viewpoint'), off (meaning 'disconnect between landmark and figure'), back (meaning 'along a line from front to back of landmark, and relatively inaccessible'), and, more rarely, away, along, and dialectally, back down (meaning 'down and behind').\\nTime (Peripheral): The period of time during which the locative relation between Figure and Ground obtains.\\nFigures (Core-Unexpressed): The Figures are items which mutually serve to identify the location of the other items.\\nDeixis (Extra-Thematic): An indication of how the locative phrase relates to the grounding of the speech situation, either locating it near (here) or far (there, yonder), or specifically separated from the speech context by distance or a barrier (over). Fillers of this FE are restricted to this closed class of items.\\nAccessibility (Extra-Thematic): An indication of how accessible the Figure is to some participant in the speech context (either the speaker or the addressee, or some other highly salient viewpoint in a narrative). This FE normally describes availability for manipulation and use, but may simply describe how easily seen or difficult to see the Figure is. There is a closed class of items that can fill this slot, including only 'just' and 'right' indicating ease of access, and 'way', 'all the way', or (dialectally) 'clear' indicating distance and difficulty of access.\\nDirectness (Extra-Thematic): An indication of how closely the position of Figure matches the prototype expected from the image schema associated with the target. This FE is normally filled by closed-class, dedicated adverbs like right, straight, directly, and due (and some combinations of these), but also occurs with rarer items like 'smack dab'.\\nTemporal_profile (Extra-Thematic): A description of how the access path to the Ground interacts with time. This FE is filled only by the closed class of adverbials back.adv (which indicates that the position was occupied by the Figure before) and on.adv (which indicates a continued path). While this FE in its most basic use refers to cases of actual motion, it can be used with descriptions of static position, in which case it either refers to fictive, imagined motion (as in the first example below), or it backgrounds motion entirely (as in the second example).\\nRegion_quantification (Extra-Thematic): A description of how much of a potential profiled region actually has (or is expected to have) a Figure in it. It is normally filled by closed-class items like 'all', 'somewhere', 'nowhere', etc.\\nProfiled_region (Core): The region within which a Figure might be found. This FE is exclusive of Figure, and is a reconstrual that is not possible with verb lexical units.\\n\\n\\n### Input:\\nThe bank will not lend money **to** the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>\", '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Goal\\nFrame Definition: A Landmark (in combination with the image schema evoked by particular targets) serves to pick out the final location of a Trajector in a construed or actual motion event.\\n\\nFrame Elements:\\nLandmark (Core): The relatively locatable entity which serves as a basis for determining the location of the Trajector using the Profiled_region.\\nProfiled_region (Core-Unexpressed): The location (located with respect to the Landmark) which is profiled by the particular image schema pertaining to the locative relation.\\nTrajector (Core): The entity which is construed to be in motion, whose location at the end of the construed motion event is in question.\\nTime (Peripheral): The time at which the Trajector is located in the Profiled_region.\\n\\n\\n### Input:\\nThe bank will not lend money **to** the company.\\n<|im_end|><|im_start|>assistant<|im_sep|>', '<|im_start|>system<|im_sep|>### Task:\\nYou are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. \\n\\n### Notes:\\n- Return the tagged sentence in a ```json ``` code block.\\n- Texts must not overlap.\\n<|im_end|><|im_start|>user<|im_sep|>### Frame Information:\\nFrame Name: Businesses\\nFrame Definition: A Proprietor owns or runs a Business which provides a Product (which may be goods or services).\\n\\nFrame Elements:\\nProduct (Peripheral): Product identifies the product or service provided by the business.\\nProprietor (Peripheral): Proprietor is the owner or chief participant in the business.\\nDescriptor (Peripheral): Descriptor indicates other characteristics of the business besides its Product and Proprietor.\\nPlace (Peripheral): This FE identifies the Place in which the business is located.\\nBusiness (Core): Business identifies the company or establishment. This may be the target itself or the Business_Name if it is an appositive not set off with commas (i.e. restrictive):\\nBusiness_name (Peripheral): The name of the Business when it appears in an appositive set off by commas (non-restrictive):\\nService_provider (Peripheral): When a Business is described in terms of the professionals who work to produce the product or service, use Service_provider.\\n\\n\\n### Input:\\nThe bank will not lend money to the **company**.\\n<|im_end|><|im_start|>assistant<|im_sep|>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c028e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 13:09:20 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LORA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoRARequest\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fsp_lora = LoRARequest(\u001b[33m\"\u001b[39m\u001b[33mfsp_lora\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m, \u001b[43mLORA_PATH\u001b[49m)\n\u001b[32m      4\u001b[39m sampling_params = SamplingParams(\n\u001b[32m      5\u001b[39m     max_tokens=MAX_TOKENS,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m responses = llm.generate(\n\u001b[32m      9\u001b[39m     messages,\n\u001b[32m     10\u001b[39m     sampling_params=sampling_params,\n\u001b[32m     11\u001b[39m     lora_request=fsp_lora\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'LORA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "fsp_lora = LoRARequest(\"fsp_lora\", 1, LORA_PATH)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "\n",
    "responses = llm.generate(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=fsp_lora\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
